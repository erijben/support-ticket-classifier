{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Support Ticket Multi-Class Classifier\n",
                "**Categories:** Billing | Technical | Account  \n",
                "**Method:** TF-IDF + Logistic Regression\n",
                "\n",
                "**Key Improvements:**\n",
                "- ✅ Label leakage mitigation: exclude weak supervision keywords from TF-IDF features\n",
                "- ✅ Cross-validation for robust evaluation\n",
                "- ✅ Train/Validation/Test split\n",
                "- ✅ Stopword removal\n",
                "- ✅ Stemming/Lemmatization\n",
                "- ✅ Emoji and non-English text handling"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import pandas as pd\n",
                "import numpy as np\n",
                "import re\n",
                "import emoji\n",
                "from sklearn.feature_extraction.text import TfidfVectorizer\n",
                "from sklearn.linear_model import LogisticRegression\n",
                "from sklearn.pipeline import Pipeline\n",
                "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\n",
                "from sklearn.metrics import (\n",
                "    accuracy_score, precision_score, recall_score, f1_score,\n",
                "    classification_report, confusion_matrix\n",
                ")\n",
                "from nltk.corpus import stopwords\n",
                "from nltk.stem import PorterStemmer\n",
                "import nltk\n",
                "import joblib\n",
                "import warnings\n",
                "warnings.filterwarnings('ignore')\n",
                "\n",
                "# Download NLTK data\n",
                "nltk.download('stopwords', quiet=True)\n",
                "print(\"Libraries loaded successfully!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Load and Filter Data"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load dataset\n",
                "df = pd.read_csv('twcs.csv')\n",
                "print(f\"Total records: {len(df)}\")\n",
                "\n",
                "# Filter inbound messages (customer messages only)\n",
                "df = df[df['inbound'] == True].copy()\n",
                "print(f\"Inbound messages: {len(df)}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Weak Supervision Labeling\n",
                "\n",
                "**Label Leakage Mitigation Strategy:**\n",
                "- Use diverse, contextual keywords for labeling\n",
                "- Later exclude these exact labeling keywords from TF-IDF feature space\n",
                "- This forces the model to learn from context and patterns, not just keyword matching"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Define labeling keywords (will be excluded from features later)\n",
                "LABELING_KEYWORDS = {\n",
                "    'billing': ['bill', 'charge', 'payment', 'invoice', 'refund', 'subscription', \n",
                "                'pay', 'card', 'credit', 'price', 'cost', 'fee', 'receipt', 'paid',\n",
                "                'transaction', 'billing', 'overcharge'],\n",
                "    \n",
                "    'technical': ['error', 'crash', 'bug', 'issue', 'problem', 'broken',\n",
                "                  'slow', 'loading', 'update', 'ios', 'android', 'app', 'website',\n",
                "                  'battery', 'freeze', 'lag', 'download', 'install', 'version',\n",
                "                  'device', 'glitch', 'wifi', 'connection'],\n",
                "    \n",
                "    'account': ['account', 'password', 'login', 'username', 'profile',\n",
                "                'reset', 'verify', 'access', 'locked', 'deactivate', 'email',\n",
                "                'register', 'authentication', 'security', 'settings']\n",
                "}\n",
                "\n",
                "def assign_label(text):\n",
                "    \"\"\"Assign label using keyword-based weak supervision. Returns None for ties.\"\"\"\n",
                "    if pd.isna(text):\n",
                "        return None\n",
                "    \n",
                "    text_lower = text.lower()\n",
                "    \n",
                "    billing_score = sum(1 for kw in LABELING_KEYWORDS['billing'] if kw in text_lower)\n",
                "    technical_score = sum(1 for kw in LABELING_KEYWORDS['technical'] if kw in text_lower)\n",
                "    account_score = sum(1 for kw in LABELING_KEYWORDS['account'] if kw in text_lower)\n",
                "    \n",
                "    scores = [billing_score, technical_score, account_score]\n",
                "    max_score = max(scores)\n",
                "    \n",
                "    # No match or tie -> None\n",
                "    if max_score == 0 or scores.count(max_score) > 1:\n",
                "        return None\n",
                "    \n",
                "    if billing_score == max_score:\n",
                "        return 'Billing'\n",
                "    elif technical_score == max_score:\n",
                "        return 'Technical'\n",
                "    else:\n",
                "        return 'Account'\n",
                "\n",
                "df['label'] = df['text'].apply(assign_label)\n",
                "\n",
                "# Drop rows with no label\n",
                "df = df[df['label'].notna()].copy()\n",
                "print(f\"\\nLabeled records: {len(df)}\")\n",
                "print(f\"\\nLabel distribution:\\n{df['label'].value_counts()}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Enhanced Text Cleaning\n",
                "\n",
                "**Improvements:**\n",
                "- Convert emojis to text descriptions\n",
                "- Remove non-English characters\n",
                "- Remove stopwords\n",
                "- Apply stemming"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Initialize tools\n",
                "stop_words = set(stopwords.words('english'))\n",
                "stemmer = PorterStemmer()\n",
                "\n",
                "def clean_text(text):\n",
                "    if pd.isna(text):\n",
                "        return \"\"\n",
                "    \n",
                "    # Convert emojis to text\n",
                "    text = emoji.demojize(text, delimiters=(\" \", \" \"))\n",
                "    \n",
                "    # Remove URLs\n",
                "    text = re.sub(r'http\\S+|www\\S+', '', text)\n",
                "    \n",
                "    # Remove mentions and hashtags\n",
                "    text = re.sub(r'@\\w+|#\\w+', '', text)\n",
                "    \n",
                "    # Keep only English characters and numbers\n",
                "    text = re.sub(r'[^a-zA-Z0-9\\s]', '', text)\n",
                "    \n",
                "    # Convert to lowercase\n",
                "    text = text.lower().strip()\n",
                "    \n",
                "    # Tokenize, remove stopwords, and stem\n",
                "    tokens = text.split()\n",
                "    tokens = [stemmer.stem(word) for word in tokens if word not in stop_words and len(word) > 2]\n",
                "    \n",
                "    # Remove extra whitespace\n",
                "    text = ' '.join(tokens)\n",
                "    \n",
                "    return text\n",
                "\n",
                "df['text_clean'] = df['text'].apply(clean_text)\n",
                "df = df[df['text_clean'].str.len() > 0].copy()\n",
                "print(f\"Records after cleaning: {len(df)}\")\n",
                "print(f\"\\nExample cleaned text:\")\n",
                "print(df[['text', 'text_clean', 'label']].head(3))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Train/Validation/Test Split\n",
                "\n",
                "**Improved data split:**\n",
                "- Use larger sample (50k instead of 20k)\n",
                "- Three-way split: 70% train, 15% validation, 15% test\n",
                "- Stratified sampling"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Use larger sample for better generalization\n",
                "SAMPLE_SIZE = min(50000, len(df))\n",
                "df_sample = df.sample(n=SAMPLE_SIZE, random_state=42)\n",
                "\n",
                "X = df_sample['text_clean']\n",
                "y = df_sample['label']\n",
                "\n",
                "# First split: train+val (85%) vs test (15%)\n",
                "X_temp, X_test, y_temp, y_test = train_test_split(\n",
                "    X, y, test_size=0.15, random_state=42, stratify=y\n",
                ")\n",
                "\n",
                "# Second split: train (70% total) vs validation (15% total)\n",
                "X_train, X_val, y_train, y_val = train_test_split(\n",
                "    X_temp, y_temp, test_size=0.176, random_state=42, stratify=y_temp  # 0.176 * 0.85 ≈ 0.15\n",
                ")\n",
                "\n",
                "print(f\"Training:   {len(X_train)} ({len(X_train)/len(X)*100:.1f}%)\")\n",
                "print(f\"Validation: {len(X_val)} ({len(X_val)/len(X)*100:.1f}%)\")\n",
                "print(f\"Test:       {len(X_test)} ({len(X_test)/len(X)*100:.1f}%)\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Custom TF-IDF Vectorizer (Label Leakage Mitigation)\n",
                "\n",
                "**Critical fix:** Exclude labeling keywords from TF-IDF features to prevent circular reasoning"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Flatten all labeling keywords\n",
                "all_labeling_keywords = set()\n",
                "for keywords in LABELING_KEYWORDS.values():\n",
                "    all_labeling_keywords.update([stemmer.stem(kw) for kw in keywords])\n",
                "\n",
                "print(f\"Excluding {len(all_labeling_keywords)} labeling keywords from features\")\n",
                "print(f\"Sample excluded keywords: {list(all_labeling_keywords)[:10]}\")\n",
                "\n",
                "# Custom stop words: English stopwords + labeling keywords\n",
                "custom_stop_words = list(stop_words) + list(all_labeling_keywords)\n",
                "\n",
                "# TF-IDF with bigrams, excluding labeling keywords\n",
                "vectorizer = TfidfVectorizer(\n",
                "    max_features=5000,\n",
                "    ngram_range=(1, 2),\n",
                "    min_df=3,\n",
                "    max_df=0.7,\n",
                "    stop_words=custom_stop_words\n",
                ")\n",
                "\n",
                "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
                "X_val_tfidf = vectorizer.transform(X_val)\n",
                "X_test_tfidf = vectorizer.transform(X_test)\n",
                "\n",
                "print(f\"\\nTF-IDF shape: {X_train_tfidf.shape}\")\n",
                "print(f\"Features extracted: {len(vectorizer.get_feature_names_out())}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. Train Model with Cross-Validation"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Train model\n",
                "model = LogisticRegression(\n",
                "    max_iter=2000,\n",
                "    solver='lbfgs',\n",
                "    class_weight='balanced',\n",
                "    random_state=42\n",
                ")\n",
                "\n",
                "print(\"Training model...\")\n",
                "model.fit(X_train_tfidf, y_train)\n",
                "print(\"Training complete!\")\n",
                "\n",
                "# Cross-validation on training set\n",
                "print(\"\\nRunning 5-Fold Cross-Validation...\")\n",
                "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
                "cv_scores = cross_val_score(model, X_train_tfidf, y_train, cv=cv, scoring='f1_macro')\n",
                "\n",
                "print(f\"\\nCross-Validation F1-Scores: {cv_scores}\")\n",
                "print(f\"Mean CV F1: {cv_scores.mean():.4f} (+/- {cv_scores.std() * 2:.4f})\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 7. Validation Set Performance"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "y_val_pred = model.predict(X_val_tfidf)\n",
                "\n",
                "val_accuracy = accuracy_score(y_val, y_val_pred)\n",
                "val_f1 = f1_score(y_val, y_val_pred, average='macro')\n",
                "\n",
                "print(\"=\"*60)\n",
                "print(\"VALIDATION SET PERFORMANCE\")\n",
                "print(\"=\"*60)\n",
                "print(f\"Accuracy:  {val_accuracy:.4f}\")\n",
                "print(f\"Macro F1:  {val_f1:.4f}\")\n",
                "print(\"\\n\" + classification_report(y_val, y_val_pred))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 8. Test Set Evaluation (Final Metrics)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "y_test_pred = model.predict(X_test_tfidf)\n",
                "\n",
                "# Fixed label order\n",
                "label_order = ['Billing', 'Technical', 'Account']\n",
                "\n",
                "# Metrics\n",
                "accuracy = accuracy_score(y_test, y_test_pred)\n",
                "macro_precision = precision_score(y_test, y_test_pred, average='macro', labels=label_order)\n",
                "macro_recall = recall_score(y_test, y_test_pred, average='macro', labels=label_order)\n",
                "macro_f1 = f1_score(y_test, y_test_pred, average='macro', labels=label_order)\n",
                "\n",
                "print(\"=\"*60)\n",
                "print(\"TEST SET PERFORMANCE (FINAL METRICS)\")\n",
                "print(\"=\"*60)\n",
                "print(f\"Accuracy:           {accuracy:.4f}\")\n",
                "print(f\"Macro Precision:    {macro_precision:.4f}\")\n",
                "print(f\"Macro Recall:       {macro_recall:.4f}\")\n",
                "print(f\"Macro F1-Score:     {macro_f1:.4f}\")\n",
                "print(\"\\n\" + \"=\"*60)\n",
                "print(\"CLASSIFICATION REPORT\")\n",
                "print(\"=\"*60)\n",
                "print(classification_report(y_test, y_test_pred, labels=label_order))\n",
                "print(\"=\"*60)\n",
                "print(\"CONFUSION MATRIX\")\n",
                "print(\"=\"*60)\n",
                "print(confusion_matrix(y_test, y_test_pred, labels=label_order))\n",
                "print(f\"\\nLabel order: {label_order}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 9. Export Model"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Create pipeline for deployment\n",
                "pipeline = Pipeline([\n",
                "    ('tfidf', vectorizer),\n",
                "    ('clf', model)\n",
                "])\n",
                "\n",
                "joblib.dump(pipeline, 'model.pkl')\n",
                "joblib.dump(vectorizer, 'tfidf_vectorizer.pkl')\n",
                "print(\"✓ Model saved as model.pkl\")\n",
                "print(\"✓ Vectorizer saved as tfidf_vectorizer.pkl\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 10. Demo: Predict on sample.csv"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load sample\n",
                "df_demo = pd.read_csv('sample.csv')\n",
                "df_demo['text_clean'] = df_demo['text'].apply(clean_text)\n",
                "df_demo = df_demo[df_demo['text_clean'].str.len() > 0]\n",
                "\n",
                "# Predict first 10\n",
                "demo_texts = df_demo['text'].head(10).values\n",
                "demo_clean = df_demo['text_clean'].head(10).values\n",
                "demo_tfidf = vectorizer.transform(demo_clean)\n",
                "predictions = model.predict(demo_tfidf)\n",
                "\n",
                "print(\"=\"*80)\n",
                "print(\"DEMO PREDICTIONS (sample.csv - First 10 rows)\")\n",
                "print(\"=\"*80)\n",
                "for i, (text, pred) in enumerate(zip(demo_texts, predictions), 1):\n",
                "    print(f\"\\n[{i}] {text[:70]}...\")\n",
                "    print(f\"    → {pred}\")\n",
                "print(\"\\n\" + \"=\"*80)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Summary of Improvements\n",
                "\n",
                "✅ **Label Leakage Mitigation**: Excluded all weak supervision keywords from TF-IDF features\n",
                "\n",
                "✅ **Validation Set**: Train/Val/Test split (70/15/15) for proper model monitoring\n",
                "\n",
                "✅ **Cross-Validation**: 5-fold stratified CV for robust performance estimation\n",
                "\n",
                "✅ **Larger Sample**: Increased from 20k to 50k samples\n",
                "\n",
                "✅ **Stopword Removal**: Using NLTK stopwords\n",
                "\n",
                "✅ **Stemming**: Porter Stemmer for normalization\n",
                "\n",
                "✅ **Emoji Handling**: Convert emojis to text descriptions\n",
                "\n",
                "✅ **Non-English Text**: Filtering to English characters only"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}