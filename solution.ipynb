{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# ðŸŽ« Support Ticket Classifier\n",
                "\n",
                "**Goal**: Classify customer support messages into Billing, Technical, or Account\n",
                "\n",
                "**Method**: TF-IDF + Logistic Regression (traditional NLP)\n",
                "\n",
                "**Improvements**:\n",
                "- âœ… Refined weak supervision labeling (better Account/Technical distinction)\n",
                "- âœ… Balanced class weights to handle imbalanced data\n",
                "- âœ… Label leakage mitigation (exclude labeling keywords from features)\n",
                "- âœ… Cross-validation for robust evaluation\n",
                "- âœ… Simple preprocessing (no external dependencies)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 22,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "âœ“ Libraries loaded successfully!\n"
                    ]
                }
            ],
            "source": [
                "import pandas as pd\n",
                "import numpy as np\n",
                "import re\n",
                "from sklearn.feature_extraction.text import TfidfVectorizer\n",
                "from sklearn.linear_model import LogisticRegression\n",
                "from sklearn.pipeline import Pipeline\n",
                "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold, GridSearchCV\n",
                "from sklearn.metrics import (\n",
                "    accuracy_score, precision_score, recall_score, f1_score,\n",
                "    classification_report, confusion_matrix\n",
                ")\n",
                "import joblib\n",
                "import warnings\n",
                "warnings.filterwarnings('ignore')\n",
                "\n",
                "print(\"âœ“ Libraries loaded successfully!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Load Data"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 23,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Total records: 2,811,774\n",
                        "Inbound customer messages: 1,537,843\n"
                    ]
                }
            ],
            "source": [
                "df = pd.read_csv('twcs.csv')\n",
                "print(f\"Total records: {len(df):,}\")\n",
                "\n",
                "# Filter inbound customer messages\n",
                "df = df[df['inbound'] == True].copy()\n",
                "print(f\"Inbound customer messages: {len(df):,}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Improved Weak Supervision Labeling\n",
                "\n",
                "**Strategy to improve Account precision**:\n",
                "- **Stronger Account signals**: Focus on authentication/access issues (login, password, locked, verify)\n",
                "- **Priority rules**: Technical errors override weak Account matches\n",
                "- **Stricter matching**: Drop ambiguous cases (ties)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 24,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "âœ“ Labeled records: 537,748\n",
                        "\n",
                        "Label distribution:\n",
                        "label\n",
                        "Technical    357426\n",
                        "Billing      158454\n",
                        "Account       21868\n",
                        "Name: count, dtype: int64\n",
                        "\n",
                        "Class proportions:\n",
                        "label\n",
                        "Technical    66.47\n",
                        "Billing      29.47\n",
                        "Account       4.07\n",
                        "Name: count, dtype: float64\n"
                    ]
                }
            ],
            "source": [
                "# Refined keyword sets with less overlap\n",
                "LABELING_KEYWORDS = {\n",
                "    'billing': [\n",
                "        'bill', 'billing', 'charge', 'charged', 'payment', 'invoice', 'refund',\n",
                "        'subscription', 'pay', 'paid', 'card', 'credit', 'debit', 'price', 'cost',\n",
                "        'fee', 'receipt', 'transaction', 'overcharge', 'cancel subscription'\n",
                "    ],\n",
                "    \n",
                "    'technical': [\n",
                "        'error', 'crash', 'crashed', 'crashing', 'bug', 'issue', 'problem',\n",
                "        'broken', 'not working', 'doesnt work', 'slow', 'loading', 'update',\n",
                "        'ios', 'android', 'app', 'website', 'battery', 'freeze', 'frozen', 'lag',\n",
                "        'download', 'install', 'version', 'device', 'glitch', 'wifi', 'connection'\n",
                "    ],\n",
                "    \n",
                "    'account': [\n",
                "        'login', 'log in', 'sign in', 'signin', 'password', 'reset password',\n",
                "        'forgot password', 'username', 'account locked', 'locked out', 'verify',\n",
                "        'verification', 'verification code', 'cant access', 'deactivate', 'reactivate',\n",
                "        'email change', '2fa', 'two factor', 'authentication', 'security code'\n",
                "    ]\n",
                "}\n",
                "\n",
                "# Strong indicators for priority-based labeling\n",
                "STRONG_TECHNICAL = ['crash', 'error', 'bug', 'not working', 'broken', 'freeze']\n",
                "STRONG_ACCOUNT = ['login', 'log in', 'password', 'locked', 'verification code', '2fa']\n",
                "\n",
                "def assign_label(text):\n",
                "    \"\"\"Priority-based labeling with reduced Account/Technical overlap\"\"\"\n",
                "    if pd.isna(text):\n",
                "        return None\n",
                "    \n",
                "    text_lower = text.lower()\n",
                "    \n",
                "    # Check strong signals first\n",
                "    has_strong_account = any(phrase in text_lower for phrase in STRONG_ACCOUNT)\n",
                "    has_strong_technical = any(phrase in text_lower for phrase in STRONG_TECHNICAL)\n",
                "    \n",
                "    # If both strong signals present, drop (ambiguous)\n",
                "    if has_strong_account and has_strong_technical:\n",
                "        return None\n",
                "    \n",
                "    # Priority: strong signals win\n",
                "    if has_strong_account:\n",
                "        return 'Account'\n",
                "    if has_strong_technical:\n",
                "        return 'Technical'\n",
                "    \n",
                "    # Default: keyword counting\n",
                "    billing_score = sum(1 for kw in LABELING_KEYWORDS['billing'] if kw in text_lower)\n",
                "    technical_score = sum(1 for kw in LABELING_KEYWORDS['technical'] if kw in text_lower)\n",
                "    account_score = sum(1 for kw in LABELING_KEYWORDS['account'] if kw in text_lower)\n",
                "    \n",
                "    scores = [billing_score, technical_score, account_score]\n",
                "    max_score = max(scores)\n",
                "    \n",
                "    # No match or tie â†’ drop\n",
                "    if max_score == 0 or scores.count(max_score) > 1:\n",
                "        return None\n",
                "    \n",
                "    if billing_score == max_score:\n",
                "        return 'Billing'\n",
                "    elif technical_score == max_score:\n",
                "        return 'Technical'\n",
                "    else:\n",
                "        return 'Account'\n",
                "\n",
                "# Apply labeling\n",
                "df['label'] = df['text'].apply(assign_label)\n",
                "df = df[df['label'].notna()].copy()\n",
                "\n",
                "print(f\"\\nâœ“ Labeled records: {len(df):,}\")\n",
                "print(f\"\\nLabel distribution:\")\n",
                "print(df['label'].value_counts())\n",
                "print(f\"\\nClass proportions:\")\n",
                "print((df['label'].value_counts() / len(df) * 100).round(2))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Simple Text Cleaning\n",
                "\n",
                "**No external dependencies** - using only basic Python and regex"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 25,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "âœ“ Records after cleaning: 534,088\n",
                        "\n",
                        "Example:\n",
                        "                                                 text  \\\n",
                        "12  @115714 yâ€™all lie about your â€œgreatâ€ connectio...   \n",
                        "23  actually that's a broken link you sent me and ...   \n",
                        "\n",
                        "                                           text_clean      label  \n",
                        "12  lie about great connection bars lte still won ...  Technical  \n",
                        "23    actually broken link sent incorrect information  Technical  \n"
                    ]
                }
            ],
            "source": [
                "# Simple English stopwords (most common)\n",
                "STOPWORDS = set([\n",
                "    'the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for',\n",
                "    'of', 'with', 'by', 'from', 'as', 'is', 'was', 'are', 'were', 'be',\n",
                "    'been', 'being', 'have', 'has', 'had', 'do', 'does', 'did', 'will',\n",
                "    'would', 'should', 'could', 'may', 'might', 'must', 'can', 'this',\n",
                "    'that', 'these', 'those', 'i', 'you', 'he', 'she', 'it', 'we', 'they',\n",
                "    'my', 'your', 'his', 'her', 'its', 'our', 'their', 'me', 'him', 'us',\n",
                "    'them', 'what', 'which', 'who', 'when', 'where', 'why', 'how', 'all',\n",
                "    'each', 'every', 'both', 'few', 'more', 'most', 'some', 'such', 'no',\n",
                "    'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 'just'\n",
                "])\n",
                "\n",
                "def clean_text(text):\n",
                "    \"\"\"Simple text cleaning without external dependencies\"\"\"\n",
                "    if pd.isna(text):\n",
                "        return \"\"\n",
                "    \n",
                "    # Lowercase\n",
                "    text = text.lower()\n",
                "    \n",
                "    # Remove URLs\n",
                "    text = re.sub(r'http\\S+|www\\S+', '', text)\n",
                "    \n",
                "    # Remove mentions and hashtags\n",
                "    text = re.sub(r'@\\w+|#\\w+', '', text)\n",
                "    \n",
                "    # Keep only letters, numbers, and spaces\n",
                "    text = re.sub(r'[^a-z0-9\\s]', ' ', text)\n",
                "    \n",
                "    # Remove extra spaces\n",
                "    text = re.sub(r'\\s+', ' ', text).strip()\n",
                "    \n",
                "    # Remove stopwords and short words\n",
                "    words = [w for w in text.split() if w not in STOPWORDS and len(w) > 2]\n",
                "    \n",
                "    return ' '.join(words)\n",
                "\n",
                "df['text_clean'] = df['text'].apply(clean_text)\n",
                "df = df[df['text_clean'].str.len() > 0].copy()\n",
                "print(f\"âœ“ Records after cleaning: {len(df):,}\")\n",
                "print(f\"\\nExample:\")\n",
                "print(df[['text', 'text_clean', 'label']].head(2))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Train/Validation/Test Split"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 26,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Training:   35,020 (70.0%)\n",
                        "Validation: 7,480 (15.0%)\n",
                        "Test:       7,500 (15.0%)\n",
                        "\n",
                        "Training set distribution:\n",
                        "label\n",
                        "Technical    23283\n",
                        "Billing      10278\n",
                        "Account       1459\n",
                        "Name: count, dtype: int64\n"
                    ]
                }
            ],
            "source": [
                "# Sample 50k for training\n",
                "SAMPLE_SIZE = min(50000, len(df))\n",
                "df_sample = df.sample(n=SAMPLE_SIZE, random_state=42)\n",
                "\n",
                "X = df_sample['text_clean']\n",
                "y = df_sample['label']\n",
                "\n",
                "# Split: 70% train, 15% validation, 15% test\n",
                "X_temp, X_test, y_temp, y_test = train_test_split(X, y, test_size=0.15, random_state=42, stratify=y)\n",
                "X_train, X_val, y_train, y_val = train_test_split(X_temp, y_temp, test_size=0.176, random_state=42, stratify=y_temp)\n",
                "\n",
                "print(f\"Training:   {len(X_train):,} ({len(X_train)/len(X)*100:.1f}%)\")\n",
                "print(f\"Validation: {len(X_val):,} ({len(X_val)/len(X)*100:.1f}%)\")\n",
                "print(f\"Test:       {len(X_test):,} ({len(X_test)/len(X)*100:.1f}%)\")\n",
                "print(f\"\\nTraining set distribution:\")\n",
                "print(pd.Series(y_train).value_counts())"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. TF-IDF Vectorization with Label Leakage Mitigation\n",
                "\n",
                "**Critical**: Exclude labeling keywords from features to prevent circular reasoning"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 27,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Excluding 81 labeling keywords from TF-IDF features\n",
                        "\n",
                        "âœ“ TF-IDF shape: (35020, 6000)\n",
                        "âœ“ Features extracted: 6000\n"
                    ]
                }
            ],
            "source": [
                "# Collect all labeling keywords to exclude\n",
                "all_labeling_keywords = set()\n",
                "for keywords in LABELING_KEYWORDS.values():\n",
                "    for kw in keywords:\n",
                "        # Add both original and space-removed versions\n",
                "        all_labeling_keywords.add(kw.replace(' ', ''))\n",
                "        all_labeling_keywords.add(kw)\n",
                "\n",
                "# Combine with stopwords\n",
                "custom_stop_words = list(STOPWORDS) + list(all_labeling_keywords)\n",
                "\n",
                "print(f\"Excluding {len(all_labeling_keywords)} labeling keywords from TF-IDF features\")\n",
                "\n",
                "# TF-IDF with bigrams\n",
                "vectorizer = TfidfVectorizer(\n",
                "    max_features=6000,\n",
                "    ngram_range=(1, 2),\n",
                "    min_df=3,\n",
                "    max_df=0.7,\n",
                "    stop_words=custom_stop_words\n",
                ")\n",
                "\n",
                "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
                "X_val_tfidf = vectorizer.transform(X_val)\n",
                "X_test_tfidf = vectorizer.transform(X_test)\n",
                "\n",
                "print(f\"\\nâœ“ TF-IDF shape: {X_train_tfidf.shape}\")\n",
                "print(f\"âœ“ Features extracted: {len(vectorizer.get_feature_names_out())}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. Train Model with Balanced Class Weights\n",
                "\n",
                "**Fix for class imbalance**: `class_weight='balanced'` automatically adjusts weights inversely proportional to class frequencies"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 28,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Running GridSearchCV with class_weight='balanced'...\n",
                        "Fitting 3 folds for each of 4 candidates, totalling 12 fits\n",
                        "\n",
                        "âœ“ Best C parameter: 2.0\n",
                        "âœ“ Best CV F1 Score: 0.7389\n"
                    ]
                }
            ],
            "source": [
                "# Light hyperparameter tuning\n",
                "param_grid = {'C': [0.5, 1.0, 2.0, 4.0]}\n",
                "\n",
                "model_base = LogisticRegression(\n",
                "    max_iter=3000,\n",
                "    solver='lbfgs',\n",
                "    class_weight='balanced',  # CRITICAL for imbalanced data\n",
                "    random_state=42\n",
                ")\n",
                "\n",
                "print(\"Running GridSearchCV with class_weight='balanced'...\")\n",
                "grid_search = GridSearchCV(\n",
                "    model_base,\n",
                "    param_grid,\n",
                "    cv=3,\n",
                "    scoring='f1_macro',\n",
                "    n_jobs=-1,\n",
                "    verbose=1\n",
                ")\n",
                "\n",
                "grid_search.fit(X_train_tfidf, y_train)\n",
                "\n",
                "print(f\"\\nâœ“ Best C parameter: {grid_search.best_params_['C']}\")\n",
                "print(f\"âœ“ Best CV F1 Score: {grid_search.best_score_:.4f}\")\n",
                "\n",
                "model = grid_search.best_estimator_"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 7. Cross-Validation"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 29,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "5-Fold Cross-Validation...\n",
                        "CV F1-Scores: [0.73687004 0.74001974 0.73464274 0.73883905 0.74456985]\n",
                        "Mean CV F1: 0.7390 (+/- 0.0067)\n"
                    ]
                }
            ],
            "source": [
                "print(\"\\n5-Fold Cross-Validation...\")\n",
                "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
                "cv_scores = cross_val_score(model, X_train_tfidf, y_train, cv=cv, scoring='f1_macro')\n",
                "\n",
                "print(f\"CV F1-Scores: {cv_scores}\")\n",
                "print(f\"Mean CV F1: {cv_scores.mean():.4f} (+/- {cv_scores.std() * 2:.4f})\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 8. Validation Set Performance"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 30,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "============================================================\n",
                        "VALIDATION SET PERFORMANCE\n",
                        "============================================================\n",
                        "Accuracy:  0.8221\n",
                        "Macro F1:  0.7544\n",
                        "\n",
                        "              precision    recall  f1-score   support\n",
                        "\n",
                        "     Account       0.55      0.77      0.64       312\n",
                        "     Billing       0.71      0.80      0.75      2195\n",
                        "   Technical       0.91      0.83      0.87      4973\n",
                        "\n",
                        "    accuracy                           0.82      7480\n",
                        "   macro avg       0.72      0.80      0.75      7480\n",
                        "weighted avg       0.84      0.82      0.83      7480\n",
                        "\n"
                    ]
                }
            ],
            "source": [
                "y_val_pred = model.predict(X_val_tfidf)\n",
                "\n",
                "print(\"=\"*60)\n",
                "print(\"VALIDATION SET PERFORMANCE\")\n",
                "print(\"=\"*60)\n",
                "print(f\"Accuracy:  {accuracy_score(y_val, y_val_pred):.4f}\")\n",
                "print(f\"Macro F1:  {f1_score(y_val, y_val_pred, average='macro'):.4f}\\n\")\n",
                "print(classification_report(y_val, y_val_pred))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 9. Test Set Evaluation (FINAL METRICS)\n",
                "\n",
                "**Required metrics**: Accuracy, Macro Precision, Macro Recall, Macro F1, Confusion Matrix, Classification Report"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 31,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "============================================================\n",
                        "TEST SET PERFORMANCE (FINAL METRICS)\n",
                        "============================================================\n",
                        "Accuracy:           0.8049 (80.49%)\n",
                        "Macro Precision:    0.6966 (69.66%)\n",
                        "Macro Recall:       0.7703 (77.03%)\n",
                        "Macro F1-Score:     0.7261 (72.61%)\n",
                        "\n",
                        "============================================================\n",
                        "CLASSIFICATION REPORT\n",
                        "============================================================\n",
                        "              precision    recall  f1-score   support\n",
                        "\n",
                        "     Billing       0.69      0.78      0.73      2201\n",
                        "   Technical       0.90      0.82      0.86      4986\n",
                        "     Account       0.50      0.71      0.59       313\n",
                        "\n",
                        "    accuracy                           0.80      7500\n",
                        "   macro avg       0.70      0.77      0.73      7500\n",
                        "weighted avg       0.82      0.80      0.81      7500\n",
                        "\n",
                        "============================================================\n",
                        "CONFUSION MATRIX\n",
                        "============================================================\n",
                        "[[1727  398   76]\n",
                        " [ 755 4089  142]\n",
                        " [  30   62  221]]\n",
                        "\n",
                        "Label order: ['Billing', 'Technical', 'Account']\n",
                        "\n",
                        "ðŸŽ¯ ACCOUNT PRECISION: 0.5034 (50.3%)\n",
                        "   Target: > 0.55 (55%)\n"
                    ]
                }
            ],
            "source": [
                "y_test_pred = model.predict(X_test_tfidf)\n",
                "\n",
                "label_order = ['Billing', 'Technical', 'Account']\n",
                "\n",
                "# Calculate metrics\n",
                "accuracy = accuracy_score(y_test, y_test_pred)\n",
                "macro_precision = precision_score(y_test, y_test_pred, average='macro', labels=label_order)\n",
                "macro_recall = recall_score(y_test, y_test_pred, average='macro', labels=label_order)\n",
                "macro_f1 = f1_score(y_test, y_test_pred, average='macro', labels=label_order)\n",
                "\n",
                "print(\"=\"*60)\n",
                "print(\"TEST SET PERFORMANCE (FINAL METRICS)\")\n",
                "print(\"=\"*60)\n",
                "print(f\"Accuracy:           {accuracy:.4f} ({accuracy*100:.2f}%)\")\n",
                "print(f\"Macro Precision:    {macro_precision:.4f} ({macro_precision*100:.2f}%)\")\n",
                "print(f\"Macro Recall:       {macro_recall:.4f} ({macro_recall*100:.2f}%)\")\n",
                "print(f\"Macro F1-Score:     {macro_f1:.4f} ({macro_f1*100:.2f}%)\")\n",
                "\n",
                "print(\"\\n\" + \"=\"*60)\n",
                "print(\"CLASSIFICATION REPORT\")\n",
                "print(\"=\"*60)\n",
                "print(classification_report(y_test, y_test_pred, labels=label_order))\n",
                "\n",
                "print(\"=\"*60)\n",
                "print(\"CONFUSION MATRIX\")\n",
                "print(\"=\"*60)\n",
                "cm = confusion_matrix(y_test, y_test_pred, labels=label_order)\n",
                "print(cm)\n",
                "print(f\"\\nLabel order: {label_order}\")\n",
                "\n",
                "# Highlight Account precision\n",
                "from sklearn.metrics import precision_recall_fscore_support\n",
                "prec, rec, f1, sup = precision_recall_fscore_support(y_test, y_test_pred, labels=label_order)\n",
                "account_idx = label_order.index('Account')\n",
                "print(f\"\\nðŸŽ¯ ACCOUNT PRECISION: {prec[account_idx]:.4f} ({prec[account_idx]*100:.1f}%)\")\n",
                "print(f\"   Target: > 0.55 (55%)\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 10. Export Model"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 32,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "âœ“ Model saved as model.pkl\n",
                        "âœ“ Vectorizer saved as tfidf_vectorizer.pkl\n"
                    ]
                }
            ],
            "source": [
                "# Create pipeline\n",
                "pipeline = Pipeline([\n",
                "    ('tfidf', vectorizer),\n",
                "    ('clf', model)\n",
                "])\n",
                "\n",
                "# Save\n",
                "joblib.dump(pipeline, 'model.pkl')\n",
                "joblib.dump(vectorizer, 'tfidf_vectorizer.pkl')\n",
                "print(\"âœ“ Model saved as model.pkl\")\n",
                "print(\"âœ“ Vectorizer saved as tfidf_vectorizer.pkl\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 11. Demo Predictions on sample.csv"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 33,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "================================================================================\n",
                        "DEMO PREDICTIONS (sample.csv - First 10 rows)\n",
                        "================================================================================\n",
                        "\n",
                        "[1] @AppleSupport causing the reply to be disregarded and the tapped notif...\n",
                        "    â†’ Predicted: Technical\n",
                        "\n",
                        "[2] @105835 Your business means a lot to us. Please DM your name, zip code...\n",
                        "    â†’ Predicted: Billing\n",
                        "\n",
                        "[3] @76328 I really hope you all change but I'm sure you won't! Because yo...\n",
                        "    â†’ Predicted: Billing\n",
                        "\n",
                        "[4] @105836 LiveChat is online at the moment - https://t.co/SY94VtU8Kq or ...\n",
                        "    â†’ Predicted: Billing\n",
                        "\n",
                        "[5] @VirginTrains see attached error message. I've tried leaving a voicema...\n",
                        "    â†’ Predicted: Account\n",
                        "\n",
                        "[6] @105836 Have you tried from another device, Miriam ^MM...\n",
                        "    â†’ Predicted: Technical\n",
                        "\n",
                        "[7] @VirginTrains yep, I've tried laptop too several times over the past w...\n",
                        "    â†’ Predicted: Technical\n",
                        "\n",
                        "[8] @105836 It's working OK from here, Miriam. Does this link help https:/...\n",
                        "    â†’ Predicted: Technical\n",
                        "\n",
                        "[9] @VirginTrains I still haven't heard &amp; the number I'm directed to b...\n",
                        "    â†’ Predicted: Technical\n",
                        "\n",
                        "[10] @105836 That's what we're here for Miriam ðŸ˜Š  The team should send you ...\n",
                        "    â†’ Predicted: Technical\n",
                        "\n",
                        "================================================================================\n",
                        "âœ“ Demo complete!\n"
                    ]
                }
            ],
            "source": [
                "# Load sample data\n",
                "df_demo = pd.read_csv('sample.csv')\n",
                "df_demo['text_clean'] = df_demo['text'].apply(clean_text)\n",
                "df_demo = df_demo[df_demo['text_clean'].str.len() > 0]\n",
                "\n",
                "# Predict first 10 rows\n",
                "demo_texts = df_demo['text'].head(10).values\n",
                "demo_clean = df_demo['text_clean'].head(10).values\n",
                "demo_tfidf = vectorizer.transform(demo_clean)\n",
                "predictions = model.predict(demo_tfidf)\n",
                "\n",
                "print(\"=\"*80)\n",
                "print(\"DEMO PREDICTIONS (sample.csv - First 10 rows)\")\n",
                "print(\"=\"*80)\n",
                "for i, (text, pred) in enumerate(zip(demo_texts, predictions), 1):\n",
                "    print(f\"\\n[{i}] {text[:70]}...\")\n",
                "    print(f\"    â†’ Predicted: {pred}\")\n",
                "print(\"\\n\" + \"=\"*80)\n",
                "print(\"âœ“ Demo complete!\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
